\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[utf8x]{inputenc}
\usepackage[letterpaper, portrait, margin=1.2in]{geometry}
\usepackage[T1]{fontenc}
\pagestyle{fancy}
\fancyhf{}
\rhead{Artem Vysogorets}
\lhead{Gram and co-Gram Points Computation}
\rfoot{Page \thepage}

\begin{document}
\section{Overview}
This recursive algorithm computes the first $N$ Gram points for arbitrary $N$ and to any a-priori given precision by solving equations of type $\theta(t)-(n-1)\pi=0$ numerically. As we shall see, the algorithm relies heavily on the fact that for $t>7$, $\theta(t)$ is a continuous, monotonic increasing function. The maximum number of recursion frames that this algorithm requires is $Log_2(sp^{-1})$, where $p$ is the specified precision, and $s$ is the initial value of a $step$ parameter.

\section{Strategy}
The underlying idea behind this algorithm is to keep ``jumping'' over the unique solution to $\theta_n(t):=\theta(t)-(n-1)\pi=0$ (i.e. the desired $n$-th Gram point), gradually decreasing the ``jump'' length ($step$) and thus converging to the solution. In the program code, $\theta_n$ is named  \textit{as\_exp\_theta}, which stands for \textit{asymptotic expansion of $\theta$-function}. In practice, the above strategy is implemented using recursion over \textit{numerical\_solution\_recursion} function, with a unique recursion frame associated with each successful ``jump''. Each frame is provided with the $x$ value that it must process (\textit{curr\_x}) and its current $step$ value. Then, this frame undertakes a series of updates of its $x$ (in $step$-increments) in the direction of decreasing $\theta_n$-function value. Provided that $\theta$-function is continuous and monotonic increasing, this direction is always towards the solution. These updates are achieved by evaluating $\theta_n$ at the following two candidate values: $curr\_x+step$ and $curr\_x-step$.  After a certain number of these updates, $x$ value must be less than one $step$ away from the solution. This occurs when $\theta_n$ evaluated at one of the two candidates with respect to the current $x$-value changes sign. When this condition is met, our frame updates $x$ value one last time and evokes itself with $x$ as the new starting $x$ value, ($curr\_x$), and $step/2$ as the new $step$. In context of this new recursion frame, these procedures are repeated, producing a nested sequence of frames that perform ``jumping'' over the Gram point by length that decreases as $Log_2$ with respect to the recursion depth. The recursion terminates when a \textit{numerical\_solution\_recursion} function frame with $step$ value smaller than precision ($prec$) is just about to jump over the solution, which implies that current $x$ value is less than one $step$ away from it. This ensures that this $x$ is within the specified error tolerance $prec$ from the actual $n$-th Gram point. Note that $step$ parameter decreases with depth of recursion, which guarantees that such a frame will be evoked and, consequently, the algorithm will always terminate.

\section{Minor Details}
Previous section provides intuition about the strategy employed by this algorithm, while some minor implementation difficulties and details are omitted. First, recall that it is crucial that $\theta$-function possesses certain properties that are only valid for, roughly speaking, $t>7$. Thus, we must ensure that $x$ values never get updated to values less than 7. This is accomplished by resetting $x$ to 7 whenever it is about to go below this mark. Additionally, it should be noted that the $precision$ requirement cannot be arbitrarily small. Even though theoretically the algorithm works for any positive value of $prec$, any attempt to actually implement it with sufficiently small $prec$ on an actual machine will run into a problem of \textit{stack overflow}---a condition when the maximum allowed recursion depth is reached. Given the maximum depth $d$ allowed for a specific machine, the best possible precision that can be achieved is $1/2^{d}s$, where $s$ is the initial step value. Based on the distribution of Gram points, $s$ can be initialized to 1 or less. However, a too small initial $step$ will significantly increase time required for the algorithm to terminate.















\end{document}

The function \textit{numerical\_solution\_recursion(curr\_x, prec, step, n)} will keep evoking itself (with updated arguments) until exiting criterion is met. First, let us discuss the procedures that take place within one recursion frame. Each frame is passed the value of $x$ (\textit{curr\_x}) that it is required to process and either exit recursion, or evoke itself with appropriate arguments. At first, three important values are computed: $i)$ value of the $as\_exp\_theta$ at $curr\_x$ (\textit{curr\_val}), $ii)$ \textit{next\_val\_plus}---value of the $as\_exp\_theta$ at $curr\_x + step$, $iii)$ \textit{next\_val\_minus}---value of the $as\_exp\_theta$ at $curr\_x - step$, 

\section{The Algorithm}
The algorithm searches for the unique solution of \textit{as\_exp\_theta(x, n)}, which is short for \textit{asymptotic expansion of Theta-function}. More formally, this function is defined as $(x/2)\log(x/2\pi) - x/2-\pi/8+1/(48x) + 7/(5760x^3) - n\pi + \pi$, so that its solution is exactly $n$-th Gram point. (from now on, we will refer to this function as to $\theta_n$). The algorithm starts with \textit{numerical\_solution} function call, which serves as a wrapper function  for the recursive function \textit{numerical\_solution\_recursion} that does all of the work. At this first stage, we need to specify the following parameters: \textit{seed}---the first value of $x$ to start with (must be > 7); \textit{prec} is the required precision, or the maximum allowed error; $n$---index of the Gram point to be computed. Next, this function evokes \textit{numerical\_solution\_recursion}, passes it all of the above parameters plus the initial \textit{step} value (the meaning of the $step$ variable will become clear in a moment). Now, the recursive part of the algorithm begins, i.e \textit{numerical\_solution\_recursion} will be evoking itself until a certain exiting condition is met. 

Roughly speaking, the goal of each recursive frame is to find $x$-value that is no farther away from the unique solution of $\theta_n$ than the current $step$ value, and then evoke itself with this new $x$-value and new $step/2$ as arguments. Thus, the algorith   This is achieved by iteratively updating $curr\_x$ to either curr_\x+step or curr_\x-step

In each recursion frame, we evaluate $\theta^{'}$ at the $x$-value (\textit{curr\_x}), which was passed by previous frames for processing. Then, we compute values of $\theta^{'}$ at $x-step$ and $x+step$

